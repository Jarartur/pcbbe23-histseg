@inproceedings{jurgasRobustMultiresolutionMultistain2024,
  title = {Robust {{Multiresolution}} and {{Multistain Background Segmentation}} in {{Whole Slide Images}}},
  booktitle = {The {{Latest Developments}} and {{Challenges}} in {{Biomedical Engineering}}},
  author = {Jurgas, Artur and Wodzinski, Marek and Atzori, Manfredo and M{\"u}ller, Henning},
  editor = {Strumi{\l}{\l}o, Pawe{\l} and Klepaczko, Artur and Strzelecki, Micha{\l} and Boci{\k a}ga, Dorota},
  year = {2024},
  series = {Lecture {{Notes}} in {{Networks}} and {{Systems}}},
  pages = {29--40},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-38430-1_3},
  abstract = {Background segmentation is an important step in analysis of histopathological images. It allows one to remove irrelevant regions and focus on the tissue of interest. However, background segmentation is challenging due to the variability of stain colors and intensity levels across different images, modalities, and magnification levels. In this paper, we present a learning-based model for histopathology background segmentation based on convolutional neural networks. We compare two multiresolution approaches to deal with the variability of magnification in histopathology images: (i) model that uses upscaling of smaller patches of the image, and (ii) model simultaneously trained on multiple resolution levels. Our model is characterized by solid performance both in multiresolution and multistain dyes (H \&E and IHC), achieving good performance on publicly available dataset. The quantitative scores are, in terms of the Dice score, close to 94.71. The qualitative analysis presents strong performance on previously unseen cases from different distributions and various dyes. We freely release the model, weights, and ground-truth annotations to promote the open science and reproducible research.},
  isbn = {978-3-031-38430-1},
  langid = {english},
  keywords = {Computational pathology,Deep learning,Digital pathology Segmentation,Whole-slide images,WSI},
}
